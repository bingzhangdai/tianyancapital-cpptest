
## Prerequisite

* cmake >= 3.14
* g++/clang++需要支持至少c++17
* 网络通畅（因为使用了一些第三方库）

## 编译运行

```bash
cmake -S . -B build
cmake --build build
cd build && ctest --output-on-failure
```

## 1. 不同券商行情处理

因为要求不论接入多少券商，printMidPrice不能修改，于是考虑封装多态。后续可以引入c++ 依赖注入的库使得代码进一步好看，`problems/q1/market_data_processor.hpp`里`processors`容器作用其实也类似于DI的容器。

由于使用虚函数，性能可能有一定损失，可以在`PrintMidPrice`函数里使用`switch-case`做静态绑定，但这样如果接入新券商，就需要在里面增加一条`case`语句，并不是很符合题目要求。

## 2. 风控设计

不同产品的风控继承自同样的基类`IRisk`，首先过滤通用的风控要求，比如禁止某些股票的交易。之后按照`accountId`来选择对应的风控模块去过滤。

所有的风控策略可以从配置文件中load起来，同时监听文件，如果文件有更新，则做热加载更新策略（代码里没实现监听热加载，因为不用实现具体风控逻辑），这样可以做到快速应用新的风控策略，并且不需要重新编译或者重启进程。

风控策略判断里加了`unique_lock`，可以视业务逻辑不加锁或者加读写锁。

## 3. 回报分发

由于订单执行系统是个独立进程，而不同的模块间没有任何关系，在`Api`里hard code `ModuleX`扩展性也不好，于是考虑进程间通信，不同模块作为独立client端订阅`Api`的消息。

进程间通信有多种方式，共享内存的方式最为高效。通过引入`sys/ipc.h`和`sys/shm.h`可以实现最基础的进程交换数据要求。然而却有以下两个问题：

1. client处理某个回报时，因为某些原因卡顿了一下（比如通过tcp连接发给策略，而这时候网络卡顿了一下），这时server端持续写入，client就可能丢失某些回报
2. client正在读的过程中，server端写覆盖了，这时读到的struct新旧数据混合（aka，读写并不是原子操作），相比于丢失，新旧数据混合可能会导致一些undefined behaviors，这里可能需要锁，然而跨进程的锁机制很麻烦。
3. 如果需求有变化，某些场景导致client和server并能在同一台机器上（比如server所在的机器上不能安装某些包，或者内存有限client不能做复杂数据处理），这时需要把client放在另一台机器上通过网络通信，代码重写太麻烦了。

对应有如下解决办法：

1. 在shared memory上做一个（环形）队列，队列就是读/写的缓冲区，只要读/写基本上速度一致，读/写进程偶尔有些卡顿慢一点点也无关紧要。同时读进程一直不停轮询，如果有数据来了立即处理，来不及处理的可以放进本进程的内存队列后续再处理。

2. 改用管道来通信。

3. 引入消息队列。ZMQ作为一个高效的消息队列，支持inproc/[ipc](http://api.zeromq.org/master:zmq-ipc)/tcp/udp等协议，底层的shared memory/named pipe/socket等通信方式对上层透明，如果要切换协议，代码也几乎不用做更改，系统扩展性更好。甚至我们可以支持不同的语言，比如某些场景client并不需要高效处理，我们可以用Python接zmq快速迭代。

这里我们采用方式3，在inproc协议下（同进程不同线程），和shared memory一致；在ipc协议下，和管道效率一致。消息的序列化我们可以交给protobuff等工具

## 4. 回报触发

题目等价于如何在`sendOrderInternal`里调用`onRtnOrder`。

